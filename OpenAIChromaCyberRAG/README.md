# Integrating OpenAIÂ LLMÂ +Â ChromaDB for Retrievalâ€‘Augmented Generation

This notebook (OpenAILLMwithChroma.ipynb) builds a complete **Retrievalâ€‘Augmentedâ€¯Generation (RAG)** pipeline.  
The notebook embeds cybersecurity data, stores vectors in **ChromaDB**, and answers questions with an OpenAI chat modelâ€”seamlessly falling back to a local Huggingâ€¯Face model when the API is unavailable, rateâ€‘limited, or too costly.


---

## ðŸ“‹ Tableâ€¯ofâ€¯Contents

1. [Project Overview](#project-overview)  
2. [Prerequisites](#prerequisites)  
3. [Script Walkthrough](#script-walkthrough)  
4. [Customization Tips](#customization-tips)  
5. [Next Steps](#next-steps)  

---

## Project Overview

Pipeline steps:

1. **Load CSV data** â€“ Read `CISA_combo_features_new.csv` withÂ `langchain`â€™s `CSVLoader`.  
2. **Generate embeddings** â€“ Encode each chunk with **MiniLM** (`allâ€‘MiniLMâ€‘L6â€‘v2`) using `HuggingFaceEmbeddings`.  
3. **Persist to ChromaDB** â€“ Add the vectors to a local Chroma collection and call `persist()` so they survive restarts.  
4. **Create a retriever** â€“ Expose the collection as a retriever with `k=1` nearestâ€‘neighbor search.  
5. **Dual Retrievalâ€‘QA chains** â€“  
   * **Primary:** OpenAIÂ GPTâ€‘3.5â€‘Turbo via `ChatOpenAI`  
   * **Secondary:** Local `gpt2` via `HuggingFacePipeline` for offlineÂ / fallback generation  
6. **Query & failâ€‘over logic** â€“ Attempts OpenAI first; on `RateLimitError` or `OpenAIError`, gracefully switches to the local model.

---

## Prerequisites

| Requirement | Minimum Version |
|-------------|-----------------|
| **Python**  | 3.9+ |
| **pip**     | 22+ |
| **Hardware**| CPU is fine; GPU (â‰¥â€¯8â€¯GBÂ VRAM) accelerates embedding & generation |

### Python packages

```bash
pip install langchain chromadb sentence-transformers pandas tqdm langchain_openai langchain_community transformers
pip install --upgrade openai
```

Set the environment variable for OpenAI:

```bash
export OPENAI_API_KEY="sk-â€¦" #SET YOUR OWN API KEY HERE
```

---

## Script Walkthrough

| Step | Purpose |
|------|---------|
| **1â€¯â€”â€¯Instantiate LLMs** | Reads `OPENAI_API_KEY` and sets up OpenAIÂ GPTâ€‘3.5 plus a local `gpt2` fallback. |
| **2â€¯â€”â€¯Build vector store** | Creates a Chroma instance backed by MiniLM embeddings, adds documents, and persists to disk. |
| **3â€¯â€”â€¯Create retriever** | Wraps the vector store for similarity search (`kâ€‘NN`). |
| **4â€¯â€”â€¯Construct Retrievalâ€‘QA chains** | Two chains share the same retriever but use different LLMsâ€”OpenAI for quality, `gpt2` for resilience. |
| **5â€¯â€”â€¯Run a query** | Executes a sample MITRE ATT&CK question with automatic fallback handling. |
| **6â€¯â€”â€¯Scale considerations** | Inline comments highlight storage, GPU, and cost factors for production deployments. |

---

## Customization Tips

* **Swap embedding model** â€“Â e.g., `paraphrase-MiniLM-L6-v2` for better paraphrase recall; just change `model_name` when instantiating `HuggingFaceEmbeddings`.  
* **Tune retrieval depth (`k`)** â€“ raise `k` in `as_retriever(search_kwargs={"k":â€¯â€¦})` for broader context windows.  
* **Persist directory** â€“ point `chromadb_path` to a mounted volume for container deployments.  
* **Replace fallback model** â€“ select a stronger openâ€‘source model (e.g., *Mistralâ€‘7Bâ€‘Instruct*) if you have GPU memory; update the `pipeline()` call accordingly.  

---

## Next Steps

* **Wrap the pipeline in a FastAPI microâ€‘service** to expose a `/query` endpoint that streams answers.  
* **Add vectorâ€‘store sharding** or switch to managed databases (e.g., Chromaâ€¯Cloud) for multiâ€‘tenant scale.  
* **Incorporate promptâ€‘engineering guards** (system messages, toolâ€‘calling) to enrich answers with provenance.  
* **Automate continuous ingestion** of fresh CISA/MITRE feeds via scheduled jobs, keeping embeddings current.  
