# -*- coding: utf-8 -*-
"""OpenAILLMwithChroma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PwZH46BqG4vQlkS57oyiIJQiwqjrQjRx

#Integrating OpenAI LLM with ChromaDB and Sentence Embedding Models

Integrating Large Language Model (LLM) with ChromaDB and a sentence embedding model using OpenAI's models. Leverage LangChain to build a retrieval-augmented generation (RAG) pipeline to efficiently retrieve relevant data and generate responses.

##Summary
This notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline that combines OpenAI’s chat‑based LLM with a local HuggingFace fallback model and ChromaDB for vector storage.
* Install and import the required packages (langchain, chromadb, sentence-transformers, etc.)
* Configure and instantiate an OpenAI-based chat model with API‑key authentication
* Build a lightweight local fallback using HuggingFace’s gpt2 pipeline
* Embed and index text data (from a CSV) into ChromaDB using MiniLM embeddings
* Split large documents into manageable chunks for retrieval
* Construct dual RetrievalQA chains (primary via OpenAI, secondary via local HF)
* Execute queries against the indexed data with automated fallback on rate limits or errors

##Prerequisites
Ensure you have the following installed:
"""

!pip install langchain chromadb sentence-transformers pandas tqdm

!pip install langchain_openai

!pip install langchain_community

!pip install transformers

!pip install --upgrade openai

"""## Step 1: Load Necessary Libraries"""

from transformers import pipeline
from langchain.llms import HuggingFacePipeline

from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.document_loaders import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

import os
from langchain_openai import ChatOpenAI

from openai import OpenAIError, RateLimitError

"""##Step 2: Read OpenAI API Key and Instantiate LLM"""

# Retrieve your OpenAI API key from the environment variable "OPENAI_API_KEY"
openai_api_key = os.getenv("OPENAI_API_KEY")
if openai_api_key is None:
    # Alternatively, you can set it directly here (not recommended for production)
    openai_api_key = "YOUR_API_KEY_HERE"

# Instantiate the ChatOpenAI LLM with your API key
llm_openai = ChatOpenAI(
    openai_api_key=openai_api_key,
    model_name="gpt-3.5-turbo",
    temperature=0,
)

"""##Step 2a: Instantiate local HuggingFace LLM text‑generation pipeline"""

hf_pipe = pipeline(
    "text-generation",
    model="gpt2",            # lightweight, no API key needed
    max_new_tokens=256, # Increased max_length to accommodate longer inputs
    do_sample=False
)
llm_fallback = HuggingFacePipeline(pipeline=hf_pipe)

"""##Step 3: Load Langchain with LLM for RetreivalQA"""

chromadb_path = './'
# Use a valid pre-trained model name instead of a local path
embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
vectordb = Chroma(persist_directory=chromadb_path, embedding_function=embedding_model)

# Create a retriever from the vector database
retriever = vectordb.as_retriever(search_kwargs={"k": 1})

"""## Step 4a: Load and Process Data"""

# 1. Load your CSV(s)
loader = CSVLoader(file_path="CISA_combo_features_new.csv", encoding="utf-8")
raw_docs = loader.load()

# 2. (Optional) Split into smaller chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(raw_docs)

"""## Step 4b: Add Documents to ChromaDB"""

# Add documents into  ChromaDB instance
vectordb.add_documents(docs)

# Persist to disk so you can re‑load later
vectordb.persist()

# Display the number of documents in the vector store
print(f"Number of documents in vector store: {vectordb._collection.count()}")

"""##Step 5a: Build RetrievalQA Chain using OpenAI model"""

# Build the RetrievalQA chain using the OpenAI model
qa_openai = RetrievalQA.from_chain_type(
    llm=llm_openai,
    chain_type="refine",
    retriever=retriever,
    return_source_documents=True,
    verbose=False,
)

"""##Step 5b: Build a Backup RetrievalQA chain pointing at the same retriever"""

qa_fallback = RetrievalQA.from_chain_type(
    llm=llm_fallback,
    chain_type="refine",
    retriever=retriever,
    return_source_documents=True,
    verbose=False,
)

"""##Step 6: Querying the System"""

query = "What is the MITRE ATT&CK technique for remote desktop attacks?"

try:
    response = qa_openai.invoke(query)
    # Print only the main result (you can also inspect source_documents if needed)
    print("✅ Response from OpenAI LLM:", response["result"])
except (RateLimitError, OpenAIError) as e:
    print(f"⚠️ OpenAI LLM failed ({e.__class__.__name__}): {e}")
    print("→ Falling back to local HuggingFace LLM")
    response = qa_fallback.invoke(query, max_new_tokens=256) # Pass max_new_tokens here
    print("✅ Fallback HF response:", response["result"])

"""##Step 7: Scaling Considerations
When scaling your solution, consider the following factors:

* Storage Needs: Plan according to the volume of data in ChromaDB.
* Performance: Use GPU-based instances for LLM operations.
* Cost Management: Monitor infrastructure costs such as AWS GPU instances.

##Conclusion
By integrating ChromaDB with LLMs, cybersecurity analysts can quickly map alerts to known techniques.
Integrated an LLM with ChromaDB and a sentence embedding model. The setup allows us to harness RAG to improve cybersecurity operations by retrieving relevant data and generating insightful responses.
Benefits:
* High accuracy & flexibility: Leverages state‑of‑the‑art OpenAI models for best‑in‑class responses while retaining a free, local LLM fallback.
* Speed & cost control: Local inference reduces latency and avoids API charges when possible.
* Scalability: ChromaDB’s persist‑and‑query architecture allows you to handle large data volumes and scale out with GPUs or managed services.
* Extensibility: Easily swap in different embedding models, vector stores, or LLMs to suit your domain.
"""